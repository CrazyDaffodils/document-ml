{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d066282",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16b11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, image_columns, image_rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a long NumPy array:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the array into a matrix where each line is an image:\n",
    "        images_matrix = all_pixels.reshape(n_images, image_columns * image_rows)\n",
    "        # Add a bias column full of 1s as the first column in the matrix\n",
    "        return np.insert(images_matrix, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63481d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60000 images, each 785 elements (1 bias + 28 * 28 pixels):\n",
    "X_train = load_images(\"./../data/mnist/train-images-idx3-ubyte.gz\")\n",
    "\n",
    "# 10000 images, each 785 elements, with the same structure as X_train:\n",
    "X_test = load_images(\"./../data/mnist/t10k-images-idx3-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25653684",
   "metadata": {},
   "source": [
    "# Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce2302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bdc379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y):\n",
    "    NUMBER_OF_CLASSES = 10        # One class per digit\n",
    "    NUMBER_OF_LABELS = Y.shape[0] # One label for each row in Y\n",
    "    \n",
    "    # Prepare a matrix of zeros with as many rows as the rows in Y,\n",
    "    # and as many columns as the number of classes:\n",
    "    encoded_labels = np.zeros((NUMBER_OF_LABELS, NUMBER_OF_CLASSES))\n",
    "\n",
    "    # For each row, flip the column that matches the label to 1:\n",
    "    for row in range(NUMBER_OF_LABELS):\n",
    "        label = Y[row]\n",
    "        encoded_labels[row][label] = 1\n",
    "        \n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "165e04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60K labels, each with value 1 if the digit is a five, and 0 otherwise:\n",
    "original_labels = load_labels(\"./../data/mnist/train-labels-idx1-ubyte.gz\")\n",
    "Y_train = one_hot_encode(original_labels)\n",
    "\n",
    "# 10000 labels, with the same encoding as Y_train:\n",
    "Y_test = load_labels(\"./../data/mnist/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b8ba3c",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd68012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], Y.shape[1]))\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration %4d \" % (i))\n",
    "        w -= np.matmul(X.T, (sigmoid(np.matmul(X, w)) - Y)) / X.shape[0] * lr\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7eedeeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 \n",
      "Iteration    1 \n",
      "Iteration    2 \n",
      "Iteration    3 \n",
      "Iteration    4 \n",
      "Iteration    5 \n",
      "Iteration    6 \n",
      "Iteration    7 \n",
      "Iteration    8 \n",
      "Iteration    9 \n",
      "Iteration   10 \n",
      "Iteration   11 \n",
      "Iteration   12 \n",
      "Iteration   13 \n",
      "Iteration   14 \n",
      "Iteration   15 \n",
      "Iteration   16 \n",
      "Iteration   17 \n",
      "Iteration   18 \n",
      "Iteration   19 \n",
      "Iteration   20 \n",
      "Iteration   21 \n",
      "Iteration   22 \n",
      "Iteration   23 \n",
      "Iteration   24 \n",
      "Iteration   25 \n",
      "Iteration   26 \n",
      "Iteration   27 \n",
      "Iteration   28 \n",
      "Iteration   29 \n",
      "Iteration   30 \n",
      "Iteration   31 \n",
      "Iteration   32 \n",
      "Iteration   33 \n",
      "Iteration   34 \n",
      "Iteration   35 \n",
      "Iteration   36 \n",
      "Iteration   37 \n",
      "Iteration   38 \n",
      "Iteration   39 \n",
      "Iteration   40 \n",
      "Iteration   41 \n",
      "Iteration   42 \n",
      "Iteration   43 \n",
      "Iteration   44 \n",
      "Iteration   45 \n",
      "Iteration   46 \n",
      "Iteration   47 \n",
      "Iteration   48 \n",
      "Iteration   49 \n",
      "Iteration   50 \n",
      "Iteration   51 \n",
      "Iteration   52 \n",
      "Iteration   53 \n",
      "Iteration   54 \n",
      "Iteration   55 \n",
      "Iteration   56 \n",
      "Iteration   57 \n",
      "Iteration   58 \n",
      "Iteration   59 \n",
      "Iteration   60 \n",
      "Iteration   61 \n",
      "Iteration   62 \n",
      "Iteration   63 \n",
      "Iteration   64 \n",
      "Iteration   65 \n",
      "Iteration   66 \n",
      "Iteration   67 \n",
      "Iteration   68 \n",
      "Iteration   69 \n",
      "Iteration   70 \n",
      "Iteration   71 \n",
      "Iteration   72 \n",
      "Iteration   73 \n",
      "Iteration   74 \n",
      "Iteration   75 \n",
      "Iteration   76 \n",
      "Iteration   77 \n",
      "Iteration   78 \n",
      "Iteration   79 \n",
      "Iteration   80 \n",
      "Iteration   81 \n",
      "Iteration   82 \n",
      "Iteration   83 \n",
      "Iteration   84 \n",
      "Iteration   85 \n",
      "Iteration   86 \n",
      "Iteration   87 \n",
      "Iteration   88 \n",
      "Iteration   89 \n",
      "Iteration   90 \n",
      "Iteration   91 \n",
      "Iteration   92 \n",
      "Iteration   93 \n",
      "Iteration   94 \n",
      "Iteration   95 \n",
      "Iteration   96 \n",
      "Iteration   97 \n",
      "Iteration   98 \n",
      "Iteration   99 \n",
      "Iteration  100 \n",
      "Iteration  101 \n",
      "Iteration  102 \n",
      "Iteration  103 \n",
      "Iteration  104 \n",
      "Iteration  105 \n",
      "Iteration  106 \n",
      "Iteration  107 \n",
      "Iteration  108 \n",
      "Iteration  109 \n",
      "Iteration  110 \n",
      "Iteration  111 \n",
      "Iteration  112 \n",
      "Iteration  113 \n",
      "Iteration  114 \n",
      "Iteration  115 \n",
      "Iteration  116 \n",
      "Iteration  117 \n",
      "Iteration  118 \n",
      "Iteration  119 \n",
      "Iteration  120 \n",
      "Iteration  121 \n",
      "Iteration  122 \n",
      "Iteration  123 \n",
      "Iteration  124 \n",
      "Iteration  125 \n",
      "Iteration  126 \n",
      "Iteration  127 \n",
      "Iteration  128 \n",
      "Iteration  129 \n",
      "Iteration  130 \n",
      "Iteration  131 \n",
      "Iteration  132 \n",
      "Iteration  133 \n",
      "Iteration  134 \n",
      "Iteration  135 \n",
      "Iteration  136 \n",
      "Iteration  137 \n",
      "Iteration  138 \n",
      "Iteration  139 \n",
      "Iteration  140 \n",
      "Iteration  141 \n",
      "Iteration  142 \n",
      "Iteration  143 \n",
      "Iteration  144 \n",
      "Iteration  145 \n",
      "Iteration  146 \n",
      "Iteration  147 \n",
      "Iteration  148 \n",
      "Iteration  149 \n",
      "Iteration  150 \n",
      "Iteration  151 \n",
      "Iteration  152 \n",
      "Iteration  153 \n",
      "Iteration  154 \n",
      "Iteration  155 \n",
      "Iteration  156 \n",
      "Iteration  157 \n",
      "Iteration  158 \n",
      "Iteration  159 \n",
      "Iteration  160 \n",
      "Iteration  161 \n",
      "Iteration  162 \n",
      "Iteration  163 \n",
      "Iteration  164 \n",
      "Iteration  165 \n",
      "Iteration  166 \n",
      "Iteration  167 \n",
      "Iteration  168 \n",
      "Iteration  169 \n",
      "Iteration  170 \n",
      "Iteration  171 \n",
      "Iteration  172 \n",
      "Iteration  173 \n",
      "Iteration  174 \n",
      "Iteration  175 \n",
      "Iteration  176 \n",
      "Iteration  177 \n",
      "Iteration  178 \n",
      "Iteration  179 \n",
      "Iteration  180 \n",
      "Iteration  181 \n",
      "Iteration  182 \n",
      "Iteration  183 \n",
      "Iteration  184 \n",
      "Iteration  185 \n",
      "Iteration  186 \n",
      "Iteration  187 \n",
      "Iteration  188 \n",
      "Iteration  189 \n",
      "Iteration  190 \n",
      "Iteration  191 \n",
      "Iteration  192 \n",
      "Iteration  193 \n",
      "Iteration  194 \n",
      "Iteration  195 \n",
      "Iteration  196 \n",
      "Iteration  197 \n",
      "Iteration  198 \n",
      "Iteration  199 \n"
     ]
    }
   ],
   "source": [
    "w = train(X_train, Y_train, iterations=200, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01b073ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    return np.argmax(sigmoid(np.matmul(X, w)), axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ee8e5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * np.count_nonzero(predict(X_test, w) == Y_test) / Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c089100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309cf9c84b6b4c41a539a63a5f66abee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675b0a216d5748c7ab349db3edf1d081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88494946d47241e6ab1b683ae7890a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404297553ad64fe99372ad6bb67cb57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab64e42285f94821833703ba18a131b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "# The tokekenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f290a554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf460d03f8844f4bb169bf332a97898e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae0f0c4d05041e19938eb0ec95201b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631b40784b4d46ad9582825e8359e8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/170 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7744687a4c492ba7d31b1af8502e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c33650a3f624b988ad9b573692a03b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488e89528fd54b6eb94df4aa4323d161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/453M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlm-base-uncased were not used when initializing LayoutLMModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing LayoutLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMTokenizer, LayoutLMModel\n",
    "import torch\n",
    "tokenizer = LayoutLMTokenizer.from_pretrained('microsoft/layoutlm-base-uncased')\n",
    "model = LayoutLMModel.from_pretrained('microsoft/layoutlm-base-uncased')\n",
    "words = [\"Hello\", \"world\"]\n",
    "normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]\n",
    "token_boxes = []\n",
    "for word, box in zip(words, normalized_word_boxes):\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    token_boxes.extend([box] * len(word_tokens))\n",
    "# add bounding boxes of cls + sep tokens\n",
    "token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
    "encoding = tokenizer(' '.join(words), return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]\n",
    "bbox = torch.tensor([token_boxes])\n",
    "outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb59b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1377, -0.0217,  0.5168,  ..., -0.3557,  0.0057, -0.0747],\n",
       "         [ 0.2349, -0.0126,  0.2175,  ...,  0.3546,  0.4783, -0.5063],\n",
       "         [ 0.1883,  0.0240,  0.5338,  ..., -0.0384,  0.2696, -0.7188],\n",
       "         [ 0.1364, -0.0190,  0.5194,  ..., -0.3599,  0.0044, -0.0714]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.1216,  0.2186, -0.0275, -0.0761,  0.2830,  0.0359, -0.4294, -0.1931,\n",
       "          0.1522,  0.0605,  0.0462,  0.2257,  0.2945, -0.3746,  0.2547,  0.1569,\n",
       "          0.4103,  0.1967, -0.0695,  0.4194, -0.3590,  0.0327,  0.0486, -0.2101,\n",
       "         -0.1370,  0.1006,  0.2010, -0.0674,  0.3664, -0.0214,  0.3499, -0.3179,\n",
       "         -0.0975,  0.2563, -0.0059, -0.4568, -0.1184, -0.2711,  0.1816,  0.0836,\n",
       "         -0.1046, -0.2034,  0.3083,  0.3592,  0.0949,  0.2845,  0.2826, -0.1538,\n",
       "         -0.3005, -0.0784, -0.0088,  0.1496, -0.3902, -0.0309, -0.4107, -0.1594,\n",
       "         -0.0673, -0.1504,  0.2526, -0.2095,  0.2628, -0.0993, -0.0671, -0.4097,\n",
       "          0.1106,  0.0872, -0.0066,  0.3961,  0.1882, -0.0712,  0.0618, -0.0669,\n",
       "          0.3061,  0.1059, -0.1721, -0.2234, -0.2344, -0.3902,  0.0178, -0.2303,\n",
       "         -0.2888,  0.0556, -0.0386,  0.2796, -0.3577,  0.2267, -0.0649,  0.0641,\n",
       "         -0.2514, -0.0288, -0.3148,  0.1810, -0.1614,  0.0983,  0.3485, -0.1784,\n",
       "          0.1936, -0.2445,  0.4223, -0.2110, -0.0572, -0.0432,  0.1959,  0.0716,\n",
       "         -0.2695,  0.1059,  0.0137, -0.2499, -0.0137,  0.0735,  0.2341,  0.0656,\n",
       "          0.0216, -0.1685, -0.2367,  0.1261, -0.2100, -0.2661, -0.1280,  0.0798,\n",
       "          0.0083,  0.2370,  0.2479, -0.0032, -0.1446, -0.0188, -0.0048,  0.3713,\n",
       "         -0.2809,  0.1688,  0.2948,  0.2414, -0.2911, -0.1683,  0.2798,  0.1585,\n",
       "         -0.2700,  0.3369,  0.1279,  0.3178,  0.2720, -0.1206,  0.1426,  0.2376,\n",
       "         -0.2815, -0.2857, -0.1143, -0.2959, -0.1124, -0.0106, -0.1273,  0.1061,\n",
       "         -0.0470, -0.2744,  0.1675, -0.0745,  0.2525,  0.0721,  0.2908, -0.1257,\n",
       "          0.1845,  0.2613, -0.2327, -0.2281,  0.1064, -0.1993, -0.3650, -0.0162,\n",
       "          0.0014, -0.0265, -0.1120, -0.2275,  0.2928, -0.1206,  0.2993,  0.0027,\n",
       "          0.1849,  0.0560, -0.1047,  0.0269,  0.2621,  0.2170, -0.2819,  0.2107,\n",
       "          0.3280, -0.2428,  0.3633,  0.0280, -0.2448, -0.3208, -0.0858, -0.0194,\n",
       "         -0.2796,  0.1889,  0.2732,  0.2787,  0.0929,  0.3536,  0.1822,  0.2447,\n",
       "         -0.0090, -0.2497,  0.2126,  0.0192, -0.0082, -0.1397,  0.0039,  0.3731,\n",
       "         -0.0630, -0.2208, -0.1295,  0.1305,  0.3129,  0.0535, -0.0135, -0.3101,\n",
       "         -0.0358, -0.1537,  0.0777,  0.1123,  0.0314, -0.0768, -0.0212, -0.1778,\n",
       "          0.1194,  0.2171, -0.1952, -0.0422,  0.0122,  0.1138,  0.0483, -0.0803,\n",
       "         -0.0368, -0.5746,  0.0738,  0.2217, -0.0189, -0.1776,  0.0505, -0.0249,\n",
       "          0.3407,  0.1666,  0.2979,  0.1487, -0.1327, -0.0439,  0.1000,  0.2851,\n",
       "          0.2415, -0.2369,  0.0329,  0.1851, -0.0589,  0.2552,  0.2448, -0.2877,\n",
       "         -0.2851, -0.4232, -0.1610,  0.3933, -0.3264, -0.1154, -0.0124, -0.2001,\n",
       "         -0.0840,  0.1083, -0.2897, -0.5070,  0.0619, -0.1881, -0.0905,  0.1841,\n",
       "          0.1724, -0.1570, -0.1477,  0.0665,  0.0428, -0.3071, -0.3061, -0.1908,\n",
       "          0.1027, -0.0591,  0.1573,  0.0958,  0.0457,  0.0670,  0.1466, -0.3340,\n",
       "         -0.2159, -0.2087, -0.0014, -0.3031,  0.4887, -0.1304,  0.2321, -0.2971,\n",
       "          0.4521,  0.1612,  0.3777, -0.2254, -0.1408,  0.3398, -0.2080,  0.4016,\n",
       "          0.4054,  0.0172, -0.1244,  0.0295, -0.0368,  0.2157, -0.0937, -0.0008,\n",
       "          0.4269,  0.0771,  0.1254,  0.0733, -0.2049,  0.1547,  0.2742, -0.1980,\n",
       "         -0.1126, -0.1483,  0.2957,  0.0770, -0.2012, -0.1826, -0.0317, -0.1411,\n",
       "          0.1621,  0.0698, -0.1501,  0.1786, -0.0505, -0.1604, -0.2121,  0.1325,\n",
       "          0.1628,  0.1911,  0.4482,  0.0388,  0.3754,  0.0415,  0.2766, -0.1297,\n",
       "          0.1085, -0.3170,  0.1150, -0.0786,  0.0143, -0.1616,  0.0979, -0.1464,\n",
       "         -0.2584, -0.2792, -0.1996, -0.0772, -0.0632, -0.0305,  0.0995,  0.2541,\n",
       "          0.0280, -0.2611,  0.3783, -0.3689,  0.3429,  0.0893, -0.1100,  0.1607,\n",
       "          0.1118, -0.0429, -0.1696, -0.1012,  0.4177, -0.2821, -0.2342,  0.1692,\n",
       "         -0.0725,  0.0086, -0.3208,  0.1541, -0.1356, -0.2386, -0.1370, -0.1702,\n",
       "          0.1265, -0.3027, -0.0213, -0.1540,  0.2850, -0.1476, -0.2863, -0.0899,\n",
       "         -0.2963,  0.0906,  0.0043,  0.0824,  0.0108, -0.3146, -0.1663, -0.0563,\n",
       "          0.2186, -0.1274,  0.3455,  0.2291, -0.2605, -0.1007, -0.3350,  0.4067,\n",
       "         -0.1156, -0.1623, -0.0021, -0.1077,  0.4045, -0.0578, -0.0267,  0.2041,\n",
       "          0.4488,  0.2856,  0.3627, -0.0935, -0.1492,  0.1100, -0.1899, -0.2082,\n",
       "         -0.3225,  0.1344, -0.2474,  0.1117, -0.0940, -0.1836, -0.1531, -0.0840,\n",
       "         -0.3958,  0.1657, -0.2378,  0.0079, -0.2147, -0.3619,  0.1376,  0.0765,\n",
       "          0.2750,  0.0225,  0.2521, -0.1359, -0.2900, -0.3091,  0.1949,  0.2407,\n",
       "         -0.3038, -0.2240, -0.0554, -0.1748,  0.0276,  0.1510, -0.3530,  0.0730,\n",
       "          0.2450, -0.1016,  0.0550,  0.3162, -0.0574,  0.2471,  0.0602,  0.0493,\n",
       "          0.3113, -0.1734,  0.1840,  0.1860,  0.0604, -0.2547, -0.1941,  0.3447,\n",
       "         -0.0645, -0.3380,  0.0937,  0.1678,  0.1865,  0.1042,  0.3250, -0.0414,\n",
       "          0.1583, -0.0789,  0.5113, -0.2181,  0.4867,  0.1625,  0.3399,  0.0764,\n",
       "         -0.1985,  0.0885,  0.0063, -0.0441,  0.1119, -0.1216, -0.2465, -0.0065,\n",
       "         -0.0789,  0.0129,  0.3187,  0.1198,  0.2145, -0.2832, -0.0249, -0.1855,\n",
       "         -0.2358,  0.0464, -0.1371, -0.1424,  0.0184, -0.1996, -0.0929, -0.3323,\n",
       "          0.2529, -0.1292,  0.1120,  0.0274, -0.0945, -0.3415,  0.3856, -0.0078,\n",
       "         -0.2815, -0.0187, -0.0553, -0.1503,  0.2277,  0.2722,  0.2933, -0.0083,\n",
       "         -0.1020, -0.6372,  0.1910, -0.1402, -0.0824, -0.0094,  0.0492, -0.1592,\n",
       "         -0.1400, -0.4047,  0.3340,  0.1343, -0.0329, -0.2422,  0.0615, -0.0682,\n",
       "         -0.2528, -0.4017,  0.0749,  0.0430, -0.3486,  0.1145, -0.3850,  0.0776,\n",
       "         -0.0420, -0.2598,  0.1199, -0.0180, -0.0631, -0.1109, -0.2550,  0.1805,\n",
       "         -0.1082, -0.0333,  0.0846,  0.0070, -0.0662, -0.1870,  0.1203,  0.0440,\n",
       "         -0.0482,  0.2048, -0.0540,  0.1611,  0.0667,  0.1123, -0.2844,  0.4749,\n",
       "          0.3578, -0.1447,  0.0017,  0.2275, -0.1317, -0.1825, -0.1028,  0.0877,\n",
       "         -0.0794,  0.2686, -0.0098,  0.2689, -0.1193, -0.1455,  0.1009, -0.1900,\n",
       "          0.2842, -0.3944,  0.1290, -0.4094, -0.1450,  0.2094, -0.2094, -0.1938,\n",
       "          0.1204, -0.3190,  0.2088, -0.4502, -0.2519,  0.1470,  0.0382, -0.0976,\n",
       "         -0.0875,  0.1495, -0.3033, -0.0420, -0.3409, -0.2293, -0.0103,  0.1146,\n",
       "         -0.1332, -0.1127, -0.2381,  0.1637, -0.0798,  0.2660, -0.2430,  0.0732,\n",
       "          0.4350, -0.1004, -0.3110, -0.2580, -0.2063, -0.2048, -0.2307,  0.0969,\n",
       "         -0.1448,  0.2164, -0.3635, -0.3319, -0.2023,  0.4636,  0.1560,  0.3377,\n",
       "         -0.1634, -0.2451,  0.2631, -0.1153, -0.1843, -0.0416, -0.2146, -0.4630,\n",
       "         -0.1106, -0.2487, -0.1051, -0.2854,  0.4339, -0.0188, -0.1167, -0.0156,\n",
       "          0.1074, -0.0974,  0.0702,  0.0774, -0.2599,  0.0415, -0.0612, -0.1530,\n",
       "          0.1504,  0.2129,  0.1440, -0.0392, -0.0509,  0.1177, -0.0432,  0.1021,\n",
       "         -0.1051, -0.0012, -0.4417, -0.4070, -0.0490, -0.0500,  0.1891, -0.0619,\n",
       "         -0.0913,  0.2019,  0.1047, -0.1563,  0.0426,  0.1567, -0.2560, -0.2326,\n",
       "          0.0177,  0.3146, -0.0267,  0.3790,  0.0550,  0.0586,  0.2373,  0.1431,\n",
       "          0.3070, -0.3694, -0.0204, -0.0110,  0.2101, -0.0539,  0.0670,  0.0384,\n",
       "         -0.1364,  0.2364,  0.2178, -0.0334,  0.1464,  0.1568,  0.0682,  0.1811,\n",
       "          0.1379,  0.0627, -0.2535,  0.3301,  0.0330,  0.1523,  0.0479, -0.1490,\n",
       "          0.3638, -0.2420, -0.4063, -0.1263,  0.3311,  0.0244, -0.1681, -0.2197,\n",
       "         -0.1052,  0.2932,  0.0026,  0.2607,  0.1503, -0.2678,  0.0998,  0.3252,\n",
       "          0.2464, -0.2572,  0.1930,  0.2087, -0.0308, -0.2809,  0.3132, -0.2073,\n",
       "          0.1860,  0.2563, -0.1654,  0.1549,  0.0318,  0.1706,  0.1901,  0.1774,\n",
       "          0.3311,  0.1281,  0.1227,  0.0832,  0.0542, -0.0766,  0.2541,  0.0600,\n",
       "          0.3581,  0.2928, -0.1784,  0.3656,  0.1572, -0.1989,  0.0097,  0.0723]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8808468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
